#!/usr/bin/env python3
"""
Train MLX Model on Custom Ticker List
=====================================
Train on specific tickers for focused intraday patterns.
"""

import json
import mlx.core as mx
import mlx.nn as nn
import numpy as np
import pandas as pd
import logging
from datetime import datetime, timedelta
from tqdm import tqdm
import os
import sys
import time
import psutil
from sklearn.preprocessing import StandardScaler

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from indicator_transformer import IndicatorLibrary
from s3_ai_db_backtest_all_tickers import DatabaseDataProvider
from optimized_avwap_indicators import OptimizedAVWAPCalculator, integrate_with_indicator_library
from train_indicator_transformer_mlx_fixed import MLXIndicatorTransformer
from mlx_trainer_fixed import MLXTrainerFixed

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')
logger = logging.getLogger(__name__)


def main():
    """Train MLX on custom ticker list."""
    import argparse
    
    parser = argparse.ArgumentParser(description='Train MLX on Custom Tickers')
    parser.add_argument('--tickers-file', default='../tickers/james_tickers.json', 
                       help='JSON file with ticker list')
    parser.add_argument('--days', type=int, default=15, help='Days of minute data')
    parser.add_argument('--epochs', type=int, default=20, help='Number of epochs')
    parser.add_argument('--batch-size', type=int, default=32, help='Batch size')
    parser.add_argument('--sample-interval', type=int, default=20, 
                       help='Minutes between samples')
    
    args = parser.parse_args()
    
    # Load tickers
    with open(args.tickers_file, 'r') as f:
        tickers = json.load(f)
    
    logger.info("\n" + "="*80)
    logger.info("ðŸš€ MLX TRAINING - CUSTOM TICKERS")
    logger.info("="*80)
    
    logger.info(f"\nðŸ“‹ Configuration:")
    logger.info(f"   Tickers: {len(tickers)} from {os.path.basename(args.tickers_file)}")
    logger.info(f"   Symbols: {', '.join(tickers[:10])}{'...' if len(tickers) > 10 else ''}")
    logger.info(f"   History: {args.days} days")
    logger.info(f"   Epochs: {args.epochs}")
    logger.info(f"   Sample interval: Every {args.sample_interval} minutes")
    
    # Initialize
    end_date = datetime.now()
    start_date = end_date - timedelta(days=args.days)
    
    db_provider = DatabaseDataProvider()
    indicator_library = IndicatorLibrary()
    indicator_library = integrate_with_indicator_library(indicator_library)
    num_indicators = len(indicator_library.indicators)
    
    logger.info(f"\nâœ… {num_indicators} indicators ready")
    
    # Process tickers
    all_samples = []
    indicator_scaler = StandardScaler()
    all_indicator_values = []
    
    logger.info(f"\nðŸ“¥ Processing {len(tickers)} tickers...")
    
    for symbol in tqdm(tickers, desc="Loading data"):
        try:
            # Get minute data
            data = db_provider.get_minute_data(symbol, start_date, end_date)
            
            if len(data) < 200:
                logger.warning(f"{symbol}: Only {len(data)} bars")
                continue
            
            # Compute indicators with optimized AVWAP
            calculator = OptimizedAVWAPCalculator(data)
            avwap_indicators = calculator.calculate_all_avwap_indicators()
            
            # All indicators
            all_indicators = np.zeros((len(data), num_indicators), dtype=np.float32)
            
            for idx, (name, func) in enumerate(indicator_library.indicators.items()):
                try:
                    if name in avwap_indicators:
                        series = avwap_indicators[name]
                    else:
                        series = func(data)
                    
                    if len(series) == len(data):
                        all_indicators[:, idx] = series.values
                    else:
                        offset = len(data) - len(series)
                        all_indicators[offset:, idx] = series.values
                except:
                    pass
            
            all_indicator_values.append(all_indicators)
            
            # Create samples
            window_size = 100
            future_window = 20
            
            for i in range(window_size, len(data) - future_window, args.sample_interval):
                try:
                    hist_data = data.iloc[i-window_size:i]
                    
                    # Market context
                    returns = hist_data['close'].pct_change().dropna()
                    market_context = np.array([
                        np.clip(returns.mean() * 10000, -100, 100),
                        np.clip(returns.std() * 10000, 0, 100),
                        np.clip(returns.skew(), -3, 3),
                        np.clip(returns.kurt(), -3, 10),
                        np.clip((hist_data['close'].iloc[-1] / hist_data['close'].iloc[0] - 1) * 10000, -500, 500),
                        np.log1p(hist_data['volume'].mean() / 1e6),
                        np.clip((hist_data['high'] - hist_data['low']).mean() / hist_data['close'].mean() * 10000, 0, 200),
                        len(returns[returns > 0]) / len(returns),
                        np.clip(returns.iloc[-10:].mean() * 10000, -100, 100),
                        np.clip(hist_data['volume'].iloc[-10:].mean() / hist_data['volume'].mean(), 0.1, 10)
                    ], dtype=np.float32)
                    
                    market_context = np.nan_to_num(market_context, 0)
                    
                    # Future return
                    future_return = (data['close'].iloc[i+future_window-1] / data['close'].iloc[i-1] - 1)
                    future_return = np.clip(future_return, -0.1, 0.1)
                    
                    # Indicator usefulness
                    usefulness = np.zeros(num_indicators, dtype=np.float32)
                    
                    for j, name in enumerate(indicator_library.indicators.keys()):
                        if 'AVWAP' in name:
                            usefulness[j] = 0.9 if 'SESSION' in name else 0.8
                        elif 'VWAP' in name:
                            usefulness[j] = 0.8
                        elif 'RSI' in name and ('7' in name or '9' in name):
                            usefulness[j] = 0.7
                        elif 'ATR' in name:
                            usefulness[j] = 0.6
                        else:
                            usefulness[j] = 0.4
                    
                    if abs(future_return) > 0.001:
                        usefulness = np.minimum(usefulness * 1.2, 1.0)
                    
                    all_samples.append({
                        'indicator_values': all_indicators[i].copy(),
                        'market_context': market_context,
                        'future_return': future_return,
                        'indicator_usefulness': usefulness
                    })
                    
                except Exception as e:
                    logger.debug(f"Sample error: {e}")
                    
        except Exception as e:
            logger.error(f"Error processing {symbol}: {e}")
    
    logger.info(f"\nâœ… Created {len(all_samples)} training samples")
    
    if len(all_samples) < 500:
        logger.error("Insufficient samples. Try more days or lower sample interval.")
        return
    
    # Normalize
    logger.info("\nðŸ“Š Normalizing indicators...")
    all_indicator_concat = np.vstack(all_indicator_values)
    indicator_scaler.fit(all_indicator_concat)
    
    for sample in all_samples:
        sample['indicator_values'] = indicator_scaler.transform(
            sample['indicator_values'].reshape(1, -1)
        ).flatten().astype(np.float32)
    
    # Split data
    np.random.shuffle(all_samples)
    val_size = int(len(all_samples) * 0.2)
    train_samples = all_samples[val_size:]
    val_samples = all_samples[:val_size]
    
    logger.info(f"   Train: {len(train_samples)} samples")
    logger.info(f"   Val: {len(val_samples)} samples")
    
    # Create MLX datasets
    class MLXDataset:
        def __init__(self, samples):
            self.samples = samples
            self.indicator_values = mx.array(
                np.array([s['indicator_values'] for s in samples], dtype=np.float32)
            )
            self.market_contexts = mx.array(
                np.array([s['market_context'] for s in samples], dtype=np.float32)
            )
            self.indicator_usefulness = mx.array(
                np.array([s['indicator_usefulness'] for s in samples], dtype=np.float32)
            )
            self.future_returns = mx.array(
                np.array([[s['future_return']] for s in samples], dtype=np.float32)
            )
            self.indicator_indices = mx.arange(self.indicator_values.shape[1])
        
        def __len__(self):
            return len(self.samples)
        
        def get_batch(self, indices):
            return {
                'indicator_values': self.indicator_values[indices],
                'market_context': self.market_contexts[indices],
                'indicator_usefulness': self.indicator_usefulness[indices],
                'future_return': self.future_returns[indices],
                'indicator_indices': mx.broadcast_to(
                    mx.expand_dims(self.indicator_indices, 0),
                    (len(indices), len(self.indicator_indices))
                )
            }
    
    train_dataset = MLXDataset(train_samples)
    val_dataset = MLXDataset(val_samples)
    
    # Create and train model
    logger.info("\nðŸ—ï¸  Creating model...")
    model = MLXIndicatorTransformer(
        num_indicators=num_indicators,
        d_model=256,
        num_heads=8,
        num_layers=6
    )
    
    logger.info("\nðŸƒ Training...")
    trainer = MLXTrainerFixed(model, learning_rate=1e-4)
    
    start_time = time.time()
    trainer.train(train_dataset, val_dataset, epochs=args.epochs, batch_size=args.batch_size)
    train_time = time.time() - start_time
    
    # Save
    import pickle
    os.makedirs('models', exist_ok=True)
    with open('models/indicator_scaler_custom.pkl', 'wb') as f:
        pickle.dump(indicator_scaler, f)
    
    logger.info(f"\n" + "="*80)
    logger.info("âœ… TRAINING COMPLETE!")
    logger.info("="*80)
    
    logger.info(f"\nðŸ“Š Summary:")
    logger.info(f"   Training time: {train_time/60:.1f} minutes")
    logger.info(f"   Samples/second: {len(train_samples) * args.epochs / train_time:.0f}")
    
    if trainer.train_losses:
        logger.info(f"\nðŸ“ˆ Final metrics:")
        logger.info(f"   Train loss: {trainer.train_losses[-1]:.4f}")
        logger.info(f"   Val loss: {min(trainer.val_losses):.4f}")
    
    logger.info(f"\nðŸŽ¯ Optimized for intraday trading with:")
    logger.info(f"   â€¢ Fast AVWAP indicators (42,000+ points/sec)")
    logger.info(f"   â€¢ {len(tickers)} liquid stocks")
    logger.info(f"   â€¢ {args.days} days of minute-level patterns")
    logger.info(f"\nðŸ“ Model saved to models/")


if __name__ == "__main__":
    main()